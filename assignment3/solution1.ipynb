{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python_projects\\nlp-innopolis\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from transformers import logging\n",
    "\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = \"DeepPavlov/rubert-base-cased\"\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "NERS_PATH = os.path.join(DATA_PATH, \"ners.txt\")\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, \"public_data\", \"train.jsonl\")\n",
    "TEST_PATH = os.path.join(DATA_PATH, \"public_data\", \"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "DR = 0.1\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL_ENTITY = \"O\"\n",
    "B_PREFIX = \"B-\"\n",
    "I_PREFIX = \"I-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(MODEL_CKPT)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NERS_PATH, \"r\") as f:\n",
    "    ner_types = [line.strip() for line in f.readlines()]\n",
    "\n",
    "ner_labels = [NULL_ENTITY] + [f\"{B_PREFIX}{ner}\" for ner in ner_types] + [f\"{I_PREFIX}{ner}\" for ner in ner_types]\n",
    "label_map = {label: i for i, label in enumerate(ner_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open(TRAIN_PATH, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        data = json.loads(line)\n",
    "        text = data['sentences']\n",
    "        ners = data['ners']\n",
    "        ners = sorted(ners, key=lambda x: x[0])\n",
    "        train_data.append({\"text\": text, \"ners\": ners})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "with open(TEST_PATH, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        data = json.loads(line)\n",
    "        text = data['senences']\n",
    "        id = data['id']\n",
    "        test_data.append({\"id\": id, \"text\":  text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding(text):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_and_labels(text, ners):\n",
    "    encoding = get_encoding(text)\n",
    "\n",
    "    token_ids = encoding[\"input_ids\"][0]\n",
    "    offset_mapping = encoding[\"offset_mapping\"][0]\n",
    "\n",
    "    ner_i = 0\n",
    "    labels = []\n",
    "\n",
    "    for offset, token_id in zip(offset_mapping, token_ids):\n",
    "\n",
    "        start, end = offset\n",
    "\n",
    "        if start == end:\n",
    "            labels.append(label_map[NULL_ENTITY])\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ner_start, ner_end, ner_type = ners[ner_i]\n",
    "        except:\n",
    "            break\n",
    "        while ner_end < start:\n",
    "            ner_i += 1\n",
    "            try:\n",
    "                ner_start, ner_end, ner_type = ners[ner_i]\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        if ner_start > end:\n",
    "            labels.append(label_map[NULL_ENTITY])\n",
    "        elif ner_start == start:\n",
    "            labels.append(label_map[B_PREFIX + ner_type])\n",
    "        else:\n",
    "            labels.append(label_map[I_PREFIX + ner_type])\n",
    "\n",
    "    return encoding, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized_data = []\n",
    "test_tokenized_data = []\n",
    "\n",
    "for sample in train_data:\n",
    "    text = sample[\"text\"]\n",
    "    ners = sample[\"ners\"]\n",
    "    train_tokenized_data.append(get_encoding_and_labels(text, ners))\n",
    "\n",
    "for sample in test_data:\n",
    "    text = sample['text']\n",
    "    id = sample['id']\n",
    "    test_tokenized_data.append((get_encoding(text), id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs, label = self.tokenized_dataset[idx]\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs, id = self.tokenized_dataset[idx]\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'offset_mapping': inputs[\"offset_mapping\"][0],\n",
    "            'id' : id\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    max_len = max(len(x) for x in input_ids)\n",
    "    padded_input_ids = torch.zeros(len(input_ids), max_len, dtype=torch.long)\n",
    "    padded_attention_mask = torch.zeros(len(attention_mask), max_len, dtype=torch.long)\n",
    "\n",
    "    for i, (input_id, mask) in enumerate(zip(input_ids, attention_mask)):\n",
    "        padded_input_ids[i, : len(input_id)] = input_id.clone().detach()\n",
    "        padded_attention_mask[i, : len(mask)] = mask.clone().detach()\n",
    "\n",
    "    max_label_len = max(len(label) for label in labels)\n",
    "    padded_labels = torch.zeros(\n",
    "        len(labels), max_label_len, len(ner_labels), dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        for j, label_id in enumerate(label):\n",
    "            padded_labels[i, j, label_id] = 1.0\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": padded_input_ids,\n",
    "        \"attention_mask\": padded_attention_mask,\n",
    "        \"labels\": padded_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_test(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    offset_mapping = [item[\"offset_mapping\"] for item in batch]\n",
    "    ids = torch.tensor([item['id'] for item in batch])\n",
    "\n",
    "    # Find the maximum length in the batch for input_ids\n",
    "    max_len = max(len(x) for x in input_ids)\n",
    "\n",
    "    # Create padded tensors for input_ids and attention_mask\n",
    "    padded_input_ids = torch.zeros(len(input_ids), max_len, dtype=torch.long)\n",
    "    padded_attention_mask = torch.zeros(len(attention_mask), max_len, dtype=torch.long)\n",
    "    padded_offset_mapping = torch.zeros(len(offset_mapping), max_len, 2, dtype=torch.long)\n",
    "\n",
    "    # Fill the padded tensors with actual data\n",
    "    for i, (input_id, mask, mapping) in enumerate(zip(input_ids, attention_mask, offset_mapping)):\n",
    "        padded_input_ids[i, :len(input_id)] = input_id.clone().detach()\n",
    "        padded_attention_mask[i, :len(mask)] = mask.clone().detach()\n",
    "        padded_offset_mapping[i, :len(mapping)] = mapping.clone().detach()\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": padded_input_ids,\n",
    "        \"attention_mask\": padded_attention_mask,\n",
    "        \"offset_mapping\": padded_offset_mapping,\n",
    "        \"ids\": ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(train_tokenized_data)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = NERTestDataset(test_tokenized_data)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.model.dropout = torch.nn.Dropout(DR)\n",
    "        self.model.classifier = torch.nn.Linear(model.config.hidden_size, len(ner_labels))\n",
    "\n",
    "        # Freeze the pretrained model weights\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for para in self.model.bert.encoder.layer[11].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Set the classifier layer weights to be trainable\n",
    "        for param in self.model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = NERModel(model).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ner_model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.096461408278522\n",
      "Epoch 2, Loss: 9.244666323942297\n",
      "Epoch 3, Loss: 8.76241050047033\n",
      "Epoch 4, Loss: 8.480520024019128\n",
      "Epoch 5, Loss: 8.336179649128633\n",
      "Epoch 6, Loss: 8.249768032747156\n",
      "Epoch 7, Loss: 8.179402716019574\n",
      "Epoch 8, Loss: 8.138147943160114\n",
      "Epoch 9, Loss: 8.097515723284554\n",
      "Epoch 10, Loss: 8.060415520387537\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    ner_model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)  # Move labels to GPU device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ner_model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model.eval()\n",
    "\n",
    "results = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    offset_mapping = batch[\"offset_mapping\"].to(device)\n",
    "    ids = batch[\"ids\"].to(device)\n",
    "\n",
    "    outputs = ner_model(input_ids, attention_mask)\n",
    "\n",
    "    predictions = torch.argmax(outputs, -1)\n",
    "\n",
    "    results += list(zip(ids, predictions, offset_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_result = []\n",
    "for id, predictions, offset_mapping  in results:\n",
    "  ners = []\n",
    "  prev_label = NULL_ENTITY\n",
    "  start_index = 0\n",
    "  end_index = 0\n",
    "  for p, m in zip(predictions, offset_mapping):\n",
    "    label = ner_labels[p]\n",
    "    if label == NULL_ENTITY:\n",
    "      if prev_label != NULL_ENTITY and start_index != end_index:\n",
    "        ners.append([start_index, end_index, prev_label[2:]])\n",
    "      prev_label = label\n",
    "    elif label[:2] == B_PREFIX:\n",
    "      if prev_label != NULL_ENTITY and start_index != end_index:\n",
    "        ners.append([start_index, end_index, prev_label[2:]])\n",
    "      prev_label = label\n",
    "      start_index = m[0].item()\n",
    "      end_index = m[1].item()\n",
    "    elif label[:2] == I_PREFIX:\n",
    "      if prev_label[2:] != label[2:]:\n",
    "        if prev_label != NULL_ENTITY and start_index != end_index:\n",
    "          ners.append([start_index, end_index, prev_label[2:]])\n",
    "        start_index = m[0].item()\n",
    "      prev_label = label\n",
    "      end_index = m[1].item()\n",
    "  res = {\"id\" : id.item(), \"ners\": ners}\n",
    "  labeled_result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.jsonl', 'w') as f:\n",
    "    for item in labeled_result:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Create a new ZIP file\n",
    "with zipfile.ZipFile('test.zip', 'w') as zip_file:\n",
    "    # Add a single file to the ZIP file\n",
    "    zip_file.write('test.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
