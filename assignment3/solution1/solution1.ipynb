{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python_projects\\nlp-innopolis\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from transformers import logging\n",
    "\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# select a device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()  # Show error messages only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = \"DeepPavlov/rubert-base-cased\"\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "NERS_PATH = os.path.join(DATA_PATH, \"ners.txt\")\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, \"public_data\", \"train.jsonl\")\n",
    "TEST_PATH = os.path.join(DATA_PATH, \"public_data\", \"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "DR = 0.1\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL_ENTITY = \"O\"  # Define the label for a non-entity token\n",
    "B_PREFIX = \"B-\"  # Define the prefix for beginning of an entity\n",
    "I_PREFIX = \"I-\"  # Define the prefix for inside an entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    MODEL_CKPT\n",
    ")  # Load the pre-trained BERT model for token classification\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    MODEL_CKPT\n",
    ")  # Load the tokenizer for the pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NERS_PATH, \"r\") as f:\n",
    "    ner_types = [\n",
    "        line.strip() for line in f.readlines()\n",
    "    ]  # Read the NER types from the specified file\n",
    "\n",
    "ner_labels = (\n",
    "    [NULL_ENTITY]\n",
    "    + [f\"{B_PREFIX}{ner}\" for ner in ner_types]\n",
    "    + [f\"{I_PREFIX}{ner}\" for ner in ner_types]\n",
    ")  # Create a list of NER labels, including the null entity and the beginning and inside entity labels\n",
    "label_map = {\n",
    "    label: i for i, label in enumerate(ner_labels)\n",
    "}  # Create a mapping from NER labels to numerical indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []  # Initialize an empty list to store the training data\n",
    "with open(TRAIN_PATH, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        data = json.loads(line)  # Load the JSON data from the training file\n",
    "        text = data[\"sentences\"]  # Extract the text from the JSON data\n",
    "        ners = data[\"ners\"]  # Extract the NER annotations from the JSON data\n",
    "        ners = sorted(\n",
    "            ners, key=lambda x: x[0]\n",
    "        )  # Sort the NER annotations based on the start position\n",
    "        train_data.append(\n",
    "            {\"text\": text, \"ners\": ners}\n",
    "        )  # Add the text and NER annotations to the train_data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []  # Initialize an empty list to store the test data\n",
    "with open(TEST_PATH, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        data = json.loads(line)  # Load the JSON data from the test file\n",
    "        text = data[\"senences\"]  # Extract the text from the JSON data\n",
    "        id = data[\"id\"]  # Extract the ID from the JSON data\n",
    "        test_data.append(\n",
    "            {\"id\": id, \"text\": text}\n",
    "        )  # Add the ID and text to the test_data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding(text):\n",
    "    \"\"\"\n",
    "    Encodes the given text using the pre-trained BERT tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be encoded.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the encoded text, attention masks, and other relevant information.\n",
    "    \"\"\"\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_and_labels(text, ners):\n",
    "    \"\"\"\n",
    "    Encodes the given text and generates corresponding NER labels based on the provided NER annotations.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        ners (list): A list of tuples, where each tuple contains (start_index, end_index, entity_type) for an NER annotation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the encoded text and a list of corresponding NER labels.\n",
    "    \"\"\"\n",
    "    encoding = get_encoding(text)  # Get the encoding for the text\n",
    "    token_ids = encoding[\"input_ids\"][0]  # Extract the token IDs from the encoding\n",
    "    offset_mapping = encoding[\"offset_mapping\"][\n",
    "        0\n",
    "    ]  # Extract the offset mapping from the encoding\n",
    "    ner_i = 0  # Initialize a counter for iterating over the NER annotations\n",
    "    labels = []  # Initialize an empty list to store the labels\n",
    "\n",
    "    for offset, token_id in zip(offset_mapping, token_ids):\n",
    "        start, end = offset  # Unpack the start and end offsets for the current token\n",
    "        if start == end:\n",
    "            labels.append(\n",
    "                label_map[NULL_ENTITY]\n",
    "            )  # If the token is a special token, assign the null entity label\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ner_start, ner_end, ner_type = ners[\n",
    "                ner_i\n",
    "            ]  # Get the start, end, and type of the current NER annotation\n",
    "        except:\n",
    "            break  # If there are no more NER annotations, break out of the loop\n",
    "\n",
    "        while ner_end < start:\n",
    "            ner_i += 1  # Move to the next NER annotation if the current one ends before the current token\n",
    "            try:\n",
    "                ner_start, ner_end, ner_type = ners[ner_i]\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        if ner_start > end:\n",
    "            labels.append(\n",
    "                label_map[NULL_ENTITY]\n",
    "            )  # If the current token is outside of any NER annotation, assign the null entity label\n",
    "        elif ner_start == start:\n",
    "            labels.append(\n",
    "                label_map[B_PREFIX + ner_type]\n",
    "            )  # If the current token is the start of an NER annotation, assign the beginning entity label\n",
    "        else:\n",
    "            labels.append(\n",
    "                label_map[I_PREFIX + ner_type]\n",
    "            )  # If the current token is inside an NER annotation, assign the inside entity label\n",
    "\n",
    "    return encoding, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store tokenized data for training and testing\n",
    "train_tokenized_data = []\n",
    "test_tokenized_data = []\n",
    "\n",
    "\n",
    "# Tokenize and encode the training data\n",
    "for sample in train_data:\n",
    "    text = sample[\"text\"]\n",
    "    ners = sample[\"ners\"]\n",
    "    train_tokenized_data.append(get_encoding_and_labels(text, ners))\n",
    "\n",
    "\n",
    "# Tokenize and encode the test data\n",
    "for sample in test_data:\n",
    "    text = sample[\"text\"]\n",
    "    id = sample[\"id\"]\n",
    "    test_tokenized_data.append((get_encoding(text), id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for NER data.\n",
    "\n",
    "    Args:\n",
    "        tokenized_dataset (list): A list of tuples containing the tokenized input and corresponding labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the tokenized input and labels for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the input IDs, attention mask, and labels as tensors.\n",
    "        \"\"\"\n",
    "        inputs, label = self.tokenized_dataset[idx]\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERTestDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for NER test data.\n",
    "\n",
    "    Args:\n",
    "        tokenized_dataset (list): A list of tuples containing the tokenized input and corresponding IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the tokenized input, offset mapping, and ID for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the input IDs, attention mask, offset mapping, and ID.\n",
    "        \"\"\"\n",
    "        inputs, id = self.tokenized_dataset[idx]\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].flatten(),\n",
    "            \"offset_mapping\": inputs[\"offset_mapping\"][0],\n",
    "            \"id\": id,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    A collate function for NER data, used to pad and batch the samples.\n",
    "\n",
    "    Args:\n",
    "        batch (list): A list of dictionaries containing the input IDs, attention masks, and labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the padded input IDs, attention masks, and labels as tensors.\n",
    "    \"\"\"\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # Find the maximum length in the batch for input_ids\n",
    "    max_len = max(len(x) for x in input_ids)\n",
    "\n",
    "    # Create padded tensors for input_ids and attention_mask\n",
    "    padded_input_ids = torch.zeros(len(input_ids), max_len, dtype=torch.long)\n",
    "    padded_attention_mask = torch.zeros(len(attention_mask), max_len, dtype=torch.long)\n",
    "\n",
    "    # Fill the padded tensors with actual data\n",
    "    for i, (input_id, mask) in enumerate(zip(input_ids, attention_mask)):\n",
    "        padded_input_ids[i, :len(input_id)] = input_id.clone().detach()\n",
    "        padded_attention_mask[i, :len(mask)] = mask.clone().detach()\n",
    "\n",
    "    # Find the maximum label length in the batch\n",
    "    max_label_len = max(len(label) for label in labels)\n",
    "\n",
    "    # Create a padded tensor for labels\n",
    "    padded_labels = torch.zeros(\n",
    "        len(labels), max_label_len, len(ner_labels), dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # Fill the padded labels tensor with actual data\n",
    "    for i, label in enumerate(labels):\n",
    "        for j, label_id in enumerate(label):\n",
    "            padded_labels[i, j, label_id] = 1.0\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": padded_input_ids,\n",
    "        \"attention_mask\": padded_attention_mask,\n",
    "        \"labels\": padded_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_test(batch):\n",
    "    \"\"\"\n",
    "    A collate function for NER test data, used to pad and batch the samples.\n",
    "\n",
    "    Args:\n",
    "        batch (list): A list of dictionaries containing the input IDs, attention masks, offset mappings, and IDs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the padded input IDs, attention masks, offset mappings, and IDs as tensors.\n",
    "    \"\"\"\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    offset_mapping = [item[\"offset_mapping\"] for item in batch]\n",
    "    ids = torch.tensor([item['id'] for item in batch])\n",
    "\n",
    "    # Find the maximum length in the batch for input_ids\n",
    "    max_len = max(len(x) for x in input_ids)\n",
    "\n",
    "    # Create padded tensors for input_ids and attention_mask\n",
    "    padded_input_ids = torch.zeros(len(input_ids), max_len, dtype=torch.long)\n",
    "    padded_attention_mask = torch.zeros(len(attention_mask), max_len, dtype=torch.long)\n",
    "    padded_offset_mapping = torch.zeros(len(offset_mapping), max_len, 2, dtype=torch.long)\n",
    "\n",
    "    # Fill the padded tensors with actual data\n",
    "    for i, (input_id, mask, mapping) in enumerate(zip(input_ids, attention_mask, offset_mapping)):\n",
    "        padded_input_ids[i, :len(input_id)] = input_id.clone().detach()\n",
    "        padded_attention_mask[i, :len(mask)] = mask.clone().detach()\n",
    "        padded_offset_mapping[i, :len(mapping)] = mapping.clone().detach()\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": padded_input_ids,\n",
    "        \"attention_mask\": padded_attention_mask,\n",
    "        \"offset_mapping\": padded_offset_mapping,\n",
    "        \"ids\": ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset for the tokenized training data\n",
    "train_dataset = NERDataset(train_tokenized_data)\n",
    "# Create a PyTorch data loader for the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Create a PyTorch dataset for the tokenized test data\n",
    "test_dataset = NERTestDataset(test_tokenized_data)\n",
    "# Create a PyTorch data loader for the test dataset\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module for the NER model.\n",
    "\n",
    "    Args:\n",
    "        model (BertForTokenClassification): The pre-trained BERT model for token classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.model.dropout = torch.nn.Dropout(DR)  # Set the dropout rate\n",
    "        self.model.classifier = torch.nn.Linear(\n",
    "            model.config.hidden_size, len(ner_labels)\n",
    "        )  # Initialize the classifier layer\n",
    "\n",
    "        # Freeze the pretrained model weights\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the weights of the last transformer layer\n",
    "        for para in self.model.bert.encoder.layer[11].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Set the classifier layer weights to be trainable\n",
    "        for param in self.model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the NER model.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): The input IDs for the BERT model.\n",
    "            attention_mask (torch.Tensor): The attention mask for the BERT model.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The logits from the NER model.\n",
    "        \"\"\"\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the NER model and move it to the GPU device\n",
    "ner_model = NERModel(model).to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(ner_model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.096461408278522\n",
      "Epoch 2, Loss: 9.244666323942297\n",
      "Epoch 3, Loss: 8.76241050047033\n",
      "Epoch 4, Loss: 8.480520024019128\n",
      "Epoch 5, Loss: 8.336179649128633\n",
      "Epoch 6, Loss: 8.249768032747156\n",
      "Epoch 7, Loss: 8.179402716019574\n",
      "Epoch 8, Loss: 8.138147943160114\n",
      "Epoch 9, Loss: 8.097515723284554\n",
      "Epoch 10, Loss: 8.060415520387537\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    ner_model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)  # Move input_ids to GPU device\n",
    "        attention_mask = batch[\"attention_mask\"].to(\n",
    "            device\n",
    "        )  # Move attention_mask to GPU device\n",
    "        labels = batch[\"labels\"].to(device)  # Move labels to GPU device\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        outputs = ner_model(input_ids, attention_mask)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate loss\n",
    "\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\"\n",
    "    )  # Print the average loss for the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "ner_model.eval()  # Set the model to evaluation mode\n",
    "results = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)  # Move input_ids to GPU device\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)  # Move attention_mask to GPU device\n",
    "    offset_mapping = batch[\"offset_mapping\"].to(device)  # Move offset_mapping to GPU device\n",
    "    ids = batch[\"ids\"].to(device)  # Move ids to GPU device\n",
    "\n",
    "    outputs = ner_model(input_ids, attention_mask)  # Forward pass\n",
    "    predictions = torch.argmax(outputs, -1)  # Get the predicted labels\n",
    "\n",
    "    # Store the results as tuples of (id, prediction, offset_mapping)\n",
    "    results += list(zip(ids, predictions, offset_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_result = []  # Initialize an empty list to store the labeled results\n",
    "\n",
    "for id, predictions, offset_mapping in results:  # Iterate over the results\n",
    "    ners = []  # Initialize an empty list to store the NER spans\n",
    "    prev_label = NULL_ENTITY  # Initialize the previous label as the null entity\n",
    "    start_index = 0  # Initialize the start index of the current NER span\n",
    "    end_index = 0  # Initialize the end index of the current NER span\n",
    "\n",
    "    for p, m in zip(\n",
    "        predictions, offset_mapping\n",
    "    ):  # Iterate over predictions and offset mappings\n",
    "        label = ner_labels[p]  # Get the label for the current prediction\n",
    "\n",
    "        if label == NULL_ENTITY:  # If the current label is the null entity\n",
    "            if prev_label != NULL_ENTITY and start_index != end_index:\n",
    "                # If a non-null entity was previously encountered, append it to the ners list\n",
    "                ners.append([start_index, end_index, prev_label[2:]])\n",
    "            prev_label = label  # Update the previous label\n",
    "\n",
    "        elif (\n",
    "            label[:2] == B_PREFIX\n",
    "        ):  # If the current label is the beginning of an entity\n",
    "            if prev_label != NULL_ENTITY and start_index != end_index:\n",
    "                # If a non-null entity was previously encountered, append it to the ners list\n",
    "                ners.append([start_index, end_index, prev_label[2:]])\n",
    "            prev_label = label  # Update the previous label\n",
    "            start_index = m[0].item()  # Update the start index of the current NER span\n",
    "            end_index = m[1].item()  # Update the end index of the current NER span\n",
    "\n",
    "        elif label[:2] == I_PREFIX:  # If the current label is inside an entity\n",
    "            if prev_label[2:] != label[2:]:\n",
    "                # If the current entity type is different from the previous entity type\n",
    "                if prev_label != NULL_ENTITY and start_index != end_index:\n",
    "                    # If a non-null entity was previously encountered, append it to the ners list\n",
    "                    ners.append([start_index, end_index, prev_label[2:]])\n",
    "                start_index = m[\n",
    "                    0\n",
    "                ].item()  # Update the start index of the current NER span\n",
    "            prev_label = label  # Update the previous label\n",
    "            end_index = m[1].item()  # Update the end index of the current NER span\n",
    "\n",
    "    # If there is a remaining non-null entity, append it to the ners list\n",
    "    if prev_label != NULL_ENTITY and start_index != end_index:\n",
    "        ners.append([start_index, end_index, prev_label[2:]])\n",
    "\n",
    "    res = {\n",
    "        \"id\": id.item(),\n",
    "        \"ners\": ners,\n",
    "    }  # Create a dictionary with the sample ID and NER spans\n",
    "    labeled_result.append(res)  # Append the result to the labeled_result list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.jsonl', 'w') as f:\n",
    "    for item in labeled_result:\n",
    "        f.write(json.dumps(item) + '\\n')  # Write each item in the labeled_result list to the test.jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"test.zip\", \"w\") as zip_file:\n",
    "    zip_file.write(\"test.jsonl\")  # Add the test.jsonl file to the ZIP file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
